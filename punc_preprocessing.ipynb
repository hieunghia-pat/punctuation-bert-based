{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hXurL3adQGAB","executionInfo":{"status":"ok","timestamp":1654104281045,"user_tz":-420,"elapsed":5338,"user":{"displayName":"Hiếu Trần","userId":"16867041673586027124"}},"outputId":"fd14502b-a31a-4f24-d0be-999d95b79058"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyvi\n","  Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n","\u001b[K     |████████████████████████████████| 8.5 MB 26.2 MB/s \n","\u001b[?25hCollecting sklearn-crfsuite\n","  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyvi) (1.0.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.1.0)\n","Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.21.6)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n","Collecting python-crfsuite>=0.8.3\n","  Downloading python_crfsuite-0.9.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (965 kB)\n","\u001b[K     |████████████████████████████████| 965 kB 45.4 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n","Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (4.64.0)\n","Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n","Successfully installed python-crfsuite-0.9.8 pyvi-0.1.1 sklearn-crfsuite-0.3.6\n"]}],"source":["!pip install pyvi\n","import os\n","import csv\n","from pyvi import ViTokenizer\n","import random\n","import re"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SJaSRM-9TGDX","executionInfo":{"status":"ok","timestamp":1654104306343,"user_tz":-420,"elapsed":18113,"user":{"displayName":"Hiếu Trần","userId":"16867041673586027124"}},"outputId":"3109802f-290e-4a1a-f05a-29310becbdd1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Convert multiple lines raw text to single line"],"metadata":{"id":"e22SqyFSSuQc"}},{"cell_type":"code","source":["DATA_TYPE = '/content/drive/MyDrive/vncorenlp/journal_hero/punctuation/Novels/generated'\n","url_pattern = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n","email_pattern = r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w+$\"\n","for name in ['qmark', 'colon', 'semicolon', 'exclam']:\n","    out_file = open(f'{DATA_TYPE}/train_{name}_line.txt', 'w', encoding='utf-8')\n","    with open(f'{DATA_TYPE}/new_{name}_data.txt', 'r', encoding='utf-8') as file:\n","        cnt = 0\n","        for line in file:\n","            line = \" \".join(line.strip().split()).replace('…', '.')\n","            line = re.sub(r'([.,-?!:;\\'~_+]){2,}', r'\\1', line)\n","            line = re.sub(r'([.,-?!:;\\'~_+]) ([.,-?!:;\\'~_+])', r'\\1', line)\n","            tokened_lines = ViTokenizer.tokenize(line.strip().lower()).replace('_', ' ')\n","            tokens = tokened_lines.split()\n","            for t in tokens:\n","                if t == '\\u200e': continue\n","                if re.search(email_pattern, t) is not None:\n","                    t = '<email_obj>'\n","                elif re.search(url_pattern, t) is not None:\n","                    t = '<url_obj>'\n","\n","                if re.search(r'(\\d[.,]?)+', t) is not None: # is number\n","                    if t.count('.') > 1 or t.count('.') > t.count(',') >= 1:\n","                        t = \" , \".join(t.replace('.', '').split(','))\n","\n","                    elif t.count(',') > 1 or 1 <= t.count('.') < t.count(','):\n","                        t = \" , \".join(t.replace(',', '').split('.'))\n","                    else:\n","                        if re.search(r'(\\d+\\,\\d{1,2})$', t) is not None:\n","                            t = \" , \".join(t.replace('.', '').split(','))\n","                        elif re.search(r'(\\d+\\.\\d{1,2})$', t) is not None:\n","                            t = \" , \".join(t.replace(',', '').split('.'))\n","                        else:\n","                            t = t.replace('.', '').replace(',','')\n","\n","                out_file.write(t + ' ')\n","    out_file.close()"],"metadata":{"id":"yGpL1mjVStsB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["punctuation_labels = {\n","    ',': 'COMMA',\n","    '.': 'PERIOD',\n","    ':': 'COLON',\n","    '?': 'QMARK',\n","    '!': 'EXCLAM',\n","    ';': 'SEMICOLON'\n","}\n","DATA_TYPE = '/content/drive/MyDrive/vncorenlp/journal_hero/punctuation/Novels/generated'\n","for prefix in ['qmark', 'colon', 'semicolon', 'exclam']:\n","    with open(f'{DATA_TYPE}/train_{prefix}.txt', 'w', encoding='utf-8') as out_file:\n","        with open(f'{DATA_TYPE}/train_{prefix}_line.txt', 'r', encoding='utf-8') as file:\n","            for line in file:\n","                while re.search(r'([.,-?!:;\\'~_+]) ([.,-?!:;\\'~_+])', line):\n","                    line = ' '.join(line.split())\n","                    line = re.sub(r'([.,-?!:;\\'~_+]) ([.,-?!:;\\'~_+])', r'\\1', line)\n","                    \n","                tokens = line.strip().split()\n","                label = None\n","                is_prev_mark = False\n","                for idx, token in enumerate(tokens):\n","                    prev_label = label\n","                    label = punctuation_labels.get(token, 'O')\n","                    if label != 'O':\n","                        out_file.write(label + '\\n')\n","                    else:\n","                        if prev_label is not None and prev_label == 'O':\n","                            out_file.write(label + '\\n')\n","                        out_file.write(token + ' ')\n"],"metadata":{"id":"oCUeG_rIUhrZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!head -n 10 $DATA_TYPE/new_exclam_data.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r5VZlWLlVM_J","executionInfo":{"status":"ok","timestamp":1654105230008,"user_tz":-420,"elapsed":326,"user":{"displayName":"Hiếu Trần","userId":"16867041673586027124"}},"outputId":"f2fe6be6-d443-4bb6-a90f-ace07f3801a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["– tiểu long cười hích hích – bọn tao có bảo mày lén uống lọ thuốc để trên đầu tủ đâu!\n","đô mi nô thuyết phục dũng cò đồng ý cho tụi mình chuộc lại thôi!\n","Anh chị này lại đi theo con mắt thứ ba nữa!\n","ừ, nhưng lần này không phải là thơ lục bát nữa đâu!\n","Nó hay đánh bạn, phải cho nó biết, nếu không nó sẽ làm nhiều tai nạn nữa!\n","chiếc xe của em buộc mấy chục quả bongbóng còn không bay lên nổi nữa là!\n","đẹp lộng lẫy!\n","Vậy đó! Thế nên, anh nói vô, mình nói đáp án vậy thôi, có nghĩ ra đâu mà hỏi!\n","Nghe đây!\n","lần này là lần chót nghen, cô giáo!\n"]}]},{"cell_type":"code","source":["!head -n 100 $DATA_TYPE/train_exclam.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fK3zxRQVCDp","executionInfo":{"status":"ok","timestamp":1654105224348,"user_tz":-420,"elapsed":8,"user":{"displayName":"Hiếu Trần","userId":"16867041673586027124"}},"outputId":"9b453c5a-5722-4ee5-d3bf-a29cca55f556"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["– O\n","tiểu O\n","long O\n","cười O\n","hích O\n","hích O\n","– O\n","bọn O\n","tao O\n","có O\n","bảo O\n","mày O\n","lén O\n","uống O\n","lọ O\n","thuốc O\n","để O\n","trên O\n","đầu O\n","tủ O\n","đâu EXCLAM\n","đô O\n","mi O\n","nô O\n","thuyết O\n","phục O\n","dũng O\n","cò O\n","đồng O\n","ý O\n","cho O\n","tụi O\n","mình O\n","chuộc O\n","lại O\n","thôi EXCLAM\n","anh O\n","chị O\n","này O\n","lại O\n","đi O\n","theo O\n","con O\n","mắt O\n","thứ O\n","ba O\n","nữa EXCLAM\n","ừ COMMA\n","nhưng O\n","lần O\n","này O\n","không O\n","phải O\n","là O\n","thơ O\n","lục O\n","bát O\n","nữa O\n","đâu EXCLAM\n","nó O\n","hay O\n","đánh O\n","bạn COMMA\n","phải O\n","cho O\n","nó O\n","biết COMMA\n","nếu O\n","không O\n","nó O\n","sẽ O\n","làm O\n","nhiều O\n","tai O\n","nạn O\n","nữa EXCLAM\n","chiếc O\n","xe O\n","của O\n","em O\n","buộc O\n","mấy O\n","chục O\n","quả O\n","bongbóng O\n","còn O\n","không O\n","bay O\n","lên O\n","nổi O\n","nữa O\n","là EXCLAM\n","đẹp O\n","lộng O\n","lẫy EXCLAM\n","vậy O\n","đó EXCLAM\n","thế O\n","nên COMMA\n","anh O\n"]}]},{"cell_type":"markdown","source":["### ???"],"metadata":{"id":"rctTjp5BS3ZT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wSYUlddgQGAI"},"outputs":[],"source":["no_dup = set()\n","with open('Subtitles/train_raw.txt', 'r', encoding='utf-8') as file:\n","    for line in file:\n","        no_dup.add(line.strip())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QFtqo19qQGAJ"},"outputs":[],"source":["out_file = open(f'Subtitles/tmp/test_raw.txt', 'w', encoding='utf-8')\n","for filename in os.listdir('Subtitles/tmp/'):\n","    if filename.endswith('srt'):\n","        with open('Subtitles/tmp/' + filename, 'r', encoding='utf-8') as file:\n","            cnt = 0\n","            for line in file:\n","                line = \" \".join(line.strip().split()).replace('…', '.')\n","                if line.count(' ') < 3:\n","                    continue\n","                if '-->' in line:\n","                    continue\n","                line = line.replace('<i>', '').replace('</i>', '')\n","                if line not in no_dup:\n","                    out_file.write(line + '\\n')\n","                    no_dup.add(line)\n","out_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbYnQRRIQGAK"},"outputs":[],"source":["for filename in os.listdir('Subtitles/tmp/'):\n","    out_file = open(f'Subtitles/tmp/test_raw.txt', 'w', encoding='utf-8')\n","    with open('Subtitles/tmp/' + filename, 'r', encoding='utf-8') as file:\n","        cnt = 0\n","        for line in file:\n","            line = \" \".join(line.strip().split()).replace('…', '.')\n","            if line.count(' ') < 3:\n","                continue\n","            if '-->' in line:\n","                continue\n","            line = re.sub(r'([.,-?!:;\\'~_+]){2,}', r'\\1', line)\n","            line = re.sub(r'([.,-?!:;\\'~_+]) ([.,-?!:;\\'~_+])', r'\\1', line)\n","            tokened_lines = ViTokenizer.tokenize(line.strip().lower()).replace('_', ' ')\n","            tokens = tokened_lines.split()\n","            for t in tokens:\n","                if re.search(email_pattern, t) is not None:\n","                    t = '<email_obj>'\n","                elif re.search(url_pattern, t) is not None:\n","                    t = '<url_obj>'\n","\n","                if re.search(r'(\\d[.,]?)+', t) is not None: # is number\n","                    if t.count('.') > 1 or t.count('.') > t.count(',') >= 1:\n","                        t = \" , \".join(t.replace('.', '').split(','))\n","\n","                    elif t.count(',') > 1 or 1 <= t.count('.') < t.count(','):\n","                        t = \" , \".join(t.replace(',', '').split('.'))\n","                    else:\n","                        if re.search(r'(\\d+\\,\\d{1,2})$', t) is not None:\n","                            t = \" , \".join(t.replace('.', '').split(','))\n","                        elif re.search(r'(\\d+\\.\\d{1,2})$', t) is not None:\n","                            t = \" , \".join(t.replace(',', '').split('.'))\n","                        else:\n","                            t = t.replace('.', '').replace(',','')\n","\n","                out_file.write(t + ' ')\n","\n","    out_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLT_oUMAQGAL"},"outputs":[],"source":["random.seed(0)\n","filenames = os.listdir('text/')\n","filenames = random.sample(filenames, int(len(filenames)*0.65))\n","len(filenames), filenames[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sJvlT1ceQGAM"},"outputs":[],"source":["train_filenames = random.sample(filenames, int(len(filenames)*0.8))\n","tmp = set(filenames)-set(train_filenames)\n","test_filenames = random.sample(list(tmp), k=int(len(tmp)*0.5))\n","valid_filenames = set(tmp)-set(test_filenames)\n","len(test_filenames), len(train_filenames), len(valid_filenames)"]},{"cell_type":"markdown","metadata":{"id":"5rOlAnMqQGAN"},"source":["### Convert to sequence tagging"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"tmL2VlevQGAO"},"outputs":[],"source":["dir_path = 'text/'\n","DATA_TYPE = 'News'\n","url_pattern = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n","email_pattern = r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w+$\"\n","for name, s in zip(['train', 'test', 'valid'], [train_filenames, test_filenames, valid_filenames]):\n","    out_file = open(f'{DATA_TYPE}/{name}_raw.txt', 'w', encoding='utf-8')\n","    for filename in s:\n","        with open(dir_path + filename, 'r', encoding='utf-8') as file:\n","            cnt = 0\n","            skip_header = True\n","            for line in file:\n","                if skip_header:\n","                    skip_header = False\n","                    continue\n","\n","                line = \" \".join(line.strip().split()).replace('…', '.')\n","                line = re.sub(r'([.,-?!:;\\'~_+]){2,}', r'\\1', line)\n","                line = re.sub(r'([.,-?!:;\\'~_+]) ([.,-?!:;\\'~_+])', r'\\1', line)\n","                tokened_lines = ViTokenizer.tokenize(line.strip().lower()).replace('_', ' ')\n","                tokens = tokened_lines.split()\n","                for t in tokens:\n","                    if re.search(email_pattern, t) is not None:\n","                        t = '<email_obj>'\n","                    elif re.search(url_pattern, t) is not None:\n","                        t = '<url_obj>'\n","\n","                    if re.search(r'(\\d[.,]?)+', t) is not None: # is number\n","                        if t.count('.') > 1 or t.count('.') > t.count(',') >= 1:\n","                            t = \" , \".join(t.replace('.', '').split(','))\n","\n","                        elif t.count(',') > 1 or 1 <= t.count('.') < t.count(','):\n","                            t = \" , \".join(t.replace(',', '').split('.'))\n","                        else:\n","                            if re.search(r'(\\d+\\,\\d{1,2})$', t) is not None:\n","                                t = \" , \".join(t.replace('.', '').split(','))\n","                            elif re.search(r'(\\d+\\.\\d{1,2})$', t) is not None:\n","                                t = \" , \".join(t.replace(',', '').split('.'))\n","                            else:\n","                                t = t.replace('.', '').replace(',','')\n","\n","                    out_file.write(t + ' ')\n","\n","    out_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeK9_HPzQGAP"},"outputs":[],"source":["DATA_TYPE = 'Subtitles'\n","url_pattern = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n","email_pattern = r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w+$\"\n","for name in ['train', 'test']:\n","    out_file = open(f'{DATA_TYPE}/{name}_all.txt', 'w', encoding='utf-8')\n","    with open(f'{DATA_TYPE}/{name}_raw.txt', 'r', encoding='utf-8') as file:\n","        cnt = 0\n","        for line in file:\n","            line = \" \".join(line.strip().split()).replace('…', '.')\n","            line = re.sub(r'([.,-?!:;\\'~_+]){2,}', r'\\1', line)\n","            line = re.sub(r'([.,-?!:;\\'~_+]) ([.,-?!:;\\'~_+])', r'\\1', line)\n","            tokened_lines = ViTokenizer.tokenize(line.strip().lower()).replace('_', ' ')\n","            tokens = tokened_lines.split()\n","            for t in tokens:\n","                if t == '\\u200e': continue\n","                if re.search(email_pattern, t) is not None:\n","                    t = '<email_obj>'\n","                elif re.search(url_pattern, t) is not None:\n","                    t = '<url_obj>'\n","\n","                if re.search(r'(\\d[.,]?)+', t) is not None: # is number\n","                    if t.count('.') > 1 or t.count('.') > t.count(',') >= 1:\n","                        t = \" , \".join(t.replace('.', '').split(','))\n","\n","                    elif t.count(',') > 1 or 1 <= t.count('.') < t.count(','):\n","                        t = \" , \".join(t.replace(',', '').split('.'))\n","                    else:\n","                        if re.search(r'(\\d+\\,\\d{1,2})$', t) is not None:\n","                            t = \" , \".join(t.replace('.', '').split(','))\n","                        elif re.search(r'(\\d+\\.\\d{1,2})$', t) is not None:\n","                            t = \" , \".join(t.replace(',', '').split('.'))\n","                        else:\n","                            t = t.replace('.', '').replace(',','')\n","\n","                out_file.write(t + ' ')\n","    out_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJrRHp-RQGAQ"},"outputs":[],"source":["punctuation_labels = {\n","    ',': 'COMMA',\n","    '.': 'PERIOD',\n","    ':': 'COLON',\n","    '?': 'QMARK',\n","    '!': 'EXCLAM',\n","    ';': 'SEMICOLON'\n","}\n","DATA_TYPE = 'Subtitles'\n","for prefix in ['test', 'train']:\n","    with open(f'{DATA_TYPE}/{prefix}.txt', 'w', encoding='utf-8') as out_file:\n","        with open(f'{DATA_TYPE}/{prefix}_all.txt', 'r', encoding='utf-8') as file:\n","            for line in file:\n","                while re.search(r'([.,-?!:;\\'~_+]) ([.,-?!:;\\'~_+])', line):\n","                    line = ' '.join(line.split())\n","                    line = re.sub(r'([.,-?!:;\\'~_+]) ([.,-?!:;\\'~_+])', r'\\1', line)\n","                    \n","                tokens = line.strip().split()\n","                label = None\n","                is_prev_mark = False\n","                for idx, token in enumerate(tokens):\n","                    prev_label = label\n","                    label = punctuation_labels.get(token, 'O')\n","                    if label != 'O':\n","                        out_file.write(label + '\\n')\n","                    else:\n","                        if prev_label is not None and prev_label == 'O':\n","                            out_file.write(label + '\\n')\n","                        out_file.write(token + ' ')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A9O2zLV6QGAQ","outputId":"8cd5d5e4-bdc9-41bf-c2f0-d8c6e48f3f72"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["with open('Subtitles/test.txt', 'r', encoding='utf-8') as file:\n","    cnt = 0\n","    tokens, labels = [], []\n","    for idx, line in enumerate(file):\n","        if ' ' not in line:\n","            print(idx)\n","            cnt += 1\n","cnt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7rDJNPB1QGAR"},"outputs":[],"source":["from collections import Counter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rh8G8YWiQGAR","outputId":"0d387d8b-ea5c-41d8-b5ee-bb6f95e804b0"},"outputs":[{"data":{"text/plain":["Counter({'O': 71789,\n","         'QMARK': 2145,\n","         'COMMA': 2652,\n","         'PERIOD': 8154,\n","         'EXCLAM': 671,\n","         'COLON': 44})"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["l = []\n","with open('Subtitles/test.txt', 'r', encoding='utf-8') as f:\n","    cnt = 0\n","    for line in f:\n","        cnt += 1\n","        if ' ' not in line:\n","            print(cnt)\n","        a, b = line.strip().split()\n","        l.append(b)\n","        \n","Counter(l)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"QiUMYRQxQGAR"},"outputs":[],"source":["\n","\n","\n","\n","end_sentence_marks = ['.', '!', '?']\n","\n","for filename in os.listdir(dir_path):\n","    with open(dir_path + filename, 'r', encoding='utf-8') as file:\n","        cnt = 0\n","        skip_header = True\n","        for line in file:\n","            if skip_header:\n","                skip_header = False\n","                continue\n","            \n","            line = \" \".join(line.strip().split())\n","            line = re.sub(r'([.,]){2,}', r'\\1', line).replace('…', '.')\n","            \n","            tokened_lines = ViTokenizer.tokenize(line.strip().lower()).replace('_', ' ')\n","            tokens = tokened_lines.split()\n","            tokens.append('<ENDLINE>')\n","            label = None\n","            is_prev_mark = True\n","            for idx, token in enumerate(tokens):\n","                label = punctuation_labels.get(token, 'O')\n","                is_write = False\n","                if not is_prev_mark:\n","                    if label == '<ENDLINE>':\n","                        out_file.write('O' + '\\n')\n","                    else:\n","                        out_file.write(label + '\\n')\n","                    is_write = True\n","                if label != 'O':\n","                    is_prev_mark = True\n","\n","                else:\n","                    is_prev_mark = False\n","                    out_file.write(token + ' ')\n","    \n","out_file.close()"]},{"cell_type":"markdown","metadata":{"id":"NGUTM2V_QGAS"},"source":["### Split comma, period number seperate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsglGq6BQGAS"},"outputs":[],"source":["punctuation_labels = {\n","    ',': 'COMMA',\n","    '.': 'PERIOD',\n","    ':': 'COLON',\n","    '?': 'QMARK',\n","    '!': 'EXCLAM',\n","    ';': 'SEMICOLON',\n","    '<ENDLINE>': '<ENDLINE>'\n","}\n","out_file = open('cleaned_BM_split.txt', 'w', encoding='utf-8')\n","with open('cleaned_BM.txt', 'r', encoding='utf-8') as file:\n","    cnt = 0\n","    for line in file:\n","        cnt += 1\n","        line = line.strip()\n","        \n","        if re.search(r'[\\.\\?\\!,:;]', line) is not None:\n","            token, label = line.split()\n","            sub_token = ''\n","            for c in token:\n","                if c not in punctuation_labels:\n","                    sub_token += c\n","                else:\n","                    out_file.write(sub_token + ' ' + punctuation_labels[c] + '\\n')\n","                    sub_token = ''\n","            if sub_token != '':\n","                out_file.write(sub_token + ' ' + label + '\\n')\n","        else:\n","            out_file.write(line + '\\n')\n","            \n","out_file.close()"]},{"cell_type":"markdown","metadata":{"id":"qbnTTGghQGAT"},"source":["### Convert unique tokens (number, email, url) into special tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q0HWB4_5QGAT"},"outputs":[],"source":["NUM = '<NUM>'\n","EMAIL = '<EMAIL>'\n","URL = '<URL>'\n","out_file = open('cleaned_BM_special.txt', 'w', encoding='utf-8')\n","with open('cleaned_BM.txt', 'r', encoding='utf-8') as file:\n","    cnt = 0\n","    for line in file:\n","        cnt += 1\n","        line = line.strip()\n","        token, label = line.split()\n","        if re.search(r'[^@]+@[^@]+\\.[^@]+', token):\n","            \n","        \n","        if re.search(r'[\\.\\?\\!,:;]', line) is not None:\n","            token, label = line.split()\n","            \n","            \n","            out_file.write(new_token + ' ' + label + '\\n')\n","        else:\n","            out_file.write(line + '\\n')\n","            \n","out_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoZS5KGHQGAT"},"outputs":[],"source":["with open('clean_news.txt', 'r', encoding='utf-8') as file:\n","    cnt = 0\n","    n_label = 0\n","    for line in file:\n","        _, l = line.strip().split()\n","        if l != 'O':\n","            n_label += 1\n","#         print(line)\n","        cnt += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KuKEaS4vQGAT"},"outputs":[],"source":["cnt, n_label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gy5--ftEQGAU"},"outputs":[],"source":["#News: 52tr tokens, 4tr labels\n","#Novels: 3tr tokens, 350k labels"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"punc_preprocessing.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}